{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Clean & Store in BigQuery_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "from pandas_gbq.gbq import TableCreationError\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _May 28, 2020_\n",
    "\n",
    "1. Get data from Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day = '2020-05-18'\n",
    "#bucket_path = f'gs://thepanacealab_covid19twitter/dailies/{day}/{day}_clean-dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_gcs(bucket_path):\n",
    "    '''\n",
    "    Takes path to file in Google Cloud Storage Bucket, and returns a Pandas DataFrame\n",
    "    '''\n",
    "    df = pd.read_json(\n",
    "        bucket_path,\n",
    "        lines=True,\n",
    "        dtype={\n",
    "            'id_str': str,\n",
    "            'in_reply_to_status_id_str': str,\n",
    "            'quoted_status_id_str': str\n",
    "        }\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df = load_from_gcs(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_bigquery(df):\n",
    "    '''\n",
    "    Takes Panda DataFrame, cleans data into appropriate form for BigQuery.\n",
    "    '''\n",
    "    cols_of_interest = [\n",
    "        'created_at',\n",
    "        'id_str',\n",
    "        'user',\n",
    "        'lang',\n",
    "        'full_text'\n",
    "    ]\n",
    "    df2clean = df.copy()[cols_of_interest]\n",
    "    df2clean['user_id_str'] = df2clean['user'].apply(lambda user: str(user['id_str']))\n",
    "    df2clean.drop(labels = 'user', axis = 1, inplace = True)\n",
    "    dfclean = df2clean[['created_at', 'id_str', 'user_id_str', 'lang', 'full_text']]\n",
    "    return dfclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#dfclean = clean_for_bigquery(df)\n",
    "#assert len(dfclean) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfclean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bq_create_table(df, day):\n",
    "#    '''\n",
    "#    Takes Pandas DataFrame, and it's associated date, and stores in BigQuery table\n",
    "#   '''\n",
    "#   bigquery_client = bigquery.Client()\n",
    "#    dataset_ref = bigquery_client.dataset('twitter_dailies')\n",
    "#    # BigQuery only accepts underscores for table names\n",
    "#    bq_day = day.replace('-', '_')\n",
    "#    # Prepares a reference to the table\n",
    "#    table_ref = dataset_ref.table(bq_day)\n",
    "#\n",
    "#    try:\n",
    "#        found = bigquery_client.get_table(table_ref)\n",
    "#        return found\n",
    "#    except Exception as e:\n",
    "#        table_ref = dataset_ref.table(bq_day)\n",
    "#        job = bigquery_client.load_table_from_dataframe(df, table_ref, location='US')\n",
    "        # waits for table load to complete\n",
    "#        job.result()\n",
    "#        print(\n",
    "#            f'Loaded dataframe from {day} with {job.output_rows} observations into',\n",
    "#            + f' {table_ref.path}'\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_create_table(df, day):\n",
    "    '''\n",
    "    Takes Pandas DataFrame, and it's associated date, and stores in BigQuery table\n",
    "    '''\n",
    "    # BigQuery only accepts underscores for table names\n",
    "    bq_day = day.replace('-', '_')\n",
    "    try:\n",
    "        pandas_gbq.to_gbq(\n",
    "            df, f'twitter_dailies.{bq_day}',\n",
    "            project_id='covid-disinfo-detect',\n",
    "            if_exists='fail'\n",
    "        )\n",
    "    except TableCreationError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#bq_create_table(dfclean, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion_bigquery(day):\n",
    "    bucket_path = (\n",
    "        f'gs://thepanacealab_covid19twitter/dailies/{day}/'\n",
    "        + f'{day}_clean-dataset.json'\n",
    "    )\n",
    "    print(f'Loading data for {day}...')\n",
    "    df = load_from_gcs(bucket_path)\n",
    "    print(f'Loaded data for {day}...')\n",
    "    dfclean = clean_for_bigquery(df)\n",
    "    print(f'Cleaned data for BigQuery...')\n",
    "    assert len(dfclean) == len(df)\n",
    "    bq_create_table(dfclean, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '2020-05-19'\n",
    "day_list = ['2020-05-25', '2020-05-24', '2020-05-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-25\n",
      "2020-05-24\n",
      "2020-05-23\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 324 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for day in day_list:\n",
    "    print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for 2020-05-25...\n",
      "Cleaned data for BigQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.04s/it]"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, dataframe, dataset_id, table_id, chunksize, schema, progress_bar)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mremaining_rows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 logger.info(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/load.py\u001b[0m in \u001b[0;36mload_chunks\u001b[0;34m(client, dataframe, dataset_id, table_id, chunksize, schema, location)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ).result()\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e3b9c81a326f>\u001b[0m in \u001b[0;36mdata_ingestion_bigquery\u001b[0;34m(day)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cleaned data for BigQuery...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfclean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbq_create_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d447fffd9887>\u001b[0m in \u001b[0;36mbq_create_table\u001b[0;34m(df, day)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'twitter_dailies.{bq_day}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'covid-disinfo-detect'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fail'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTableCreationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, dataframe, dataset_id, table_id, chunksize, schema, progress_bar)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# <https://cloud.google.com/bigquery/troubleshooting-errors>`__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     def run_query(\n",
      "\u001b[0;31mGenericGBQException\u001b[0m: Reason: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for day in day_list:\n",
    "    data_ingestion_bigquery(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#utils.download_json('2020-05-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets in covid-disinfo-detect:\n",
      "\tcovid-disinfo-detect.appspot.com\n",
      "\tstaging.covid-disinfo-detect.appspot.com\n",
      "\tthepanacealab_covid19twitter\n"
     ]
    }
   ],
   "source": [
    "sb_client = storage.Client()\n",
    "buckets = sb_client.list_buckets()\n",
    "\n",
    "print(\"Buckets in {}:\".format(sb_client.project))\n",
    "for item in buckets:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "        \n",
    "start_dt = date(2015, 12, 20)\n",
    "end_dt = date(2016, 1, 11)\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    print(dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_exists(day, bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(f'dailies/{day}/{day}_clean-dataset.json')\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_bucket(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix='dailies/', suff))\n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blobs_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_exists('2020-05-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_tables_list():\n",
    "    # gather list of tables in bigquery\n",
    "    client = bigquery.Client()\n",
    "    dataset = client.get_dataset('twitter_dailies')\n",
    "    tables = list(client.list_tables(dataset))\n",
    "    return [table.table_id for table in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_tables = bigquery_tables_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_bucket(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix='dailies/')\n",
    "    json_files = [\n",
    "        str(i).split(',')[1].strip() for i in blobs if str(i).split(',')[1].endswith('.json')\n",
    "    ]\n",
    "    storage_dates = [i.split('/')[1].replace('-', '_') for i in json_files]\n",
    "    return json_files, storage_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files, storage_dates = list_blobs_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_05_19', '2020_05_20']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigquery_tables_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_03_22',\n",
       " '2020_03_23',\n",
       " '2020_03_24',\n",
       " '2020_03_25',\n",
       " '2020_03_26',\n",
       " '2020_03_27',\n",
       " '2020_03_28',\n",
       " '2020_03_29',\n",
       " '2020_03_30',\n",
       " '2020_03_31',\n",
       " '2020_04_01',\n",
       " '2020_04_02',\n",
       " '2020_04_03',\n",
       " '2020_04_04',\n",
       " '2020_04_05',\n",
       " '2020_04_06',\n",
       " '2020_04_07',\n",
       " '2020_04_08',\n",
       " '2020_04_09',\n",
       " '2020_04_10',\n",
       " '2020_04_11',\n",
       " '2020_04_12',\n",
       " '2020_04_13',\n",
       " '2020_04_14',\n",
       " '2020_04_15',\n",
       " '2020_04_16',\n",
       " '2020_04_17',\n",
       " '2020_04_18',\n",
       " '2020_04_19',\n",
       " '2020_04_20',\n",
       " '2020_04_21',\n",
       " '2020_04_22',\n",
       " '2020_04_23',\n",
       " '2020_04_24',\n",
       " '2020_04_25',\n",
       " '2020_04_26',\n",
       " '2020_04_27',\n",
       " '2020_04_28',\n",
       " '2020_04_29',\n",
       " '2020_04_30',\n",
       " '2020_05_01',\n",
       " '2020_05_02',\n",
       " '2020_05_03',\n",
       " '2020_05_04',\n",
       " '2020_05_05',\n",
       " '2020_05_06',\n",
       " '2020_05_07',\n",
       " '2020_05_08',\n",
       " '2020_05_09',\n",
       " '2020_05_10',\n",
       " '2020_05_11',\n",
       " '2020_05_12',\n",
       " '2020_05_13',\n",
       " '2020_05_14',\n",
       " '2020_05_15',\n",
       " '2020_05_16',\n",
       " '2020_05_17',\n",
       " '2020_05_18',\n",
       " '2020_05_19',\n",
       " '2020_05_20',\n",
       " '2020_05_21',\n",
       " '2020_05_22',\n",
       " '2020_05_23']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_03_22',\n",
       " '2020_03_23',\n",
       " '2020_03_24',\n",
       " '2020_03_25',\n",
       " '2020_03_26',\n",
       " '2020_03_27',\n",
       " '2020_03_28',\n",
       " '2020_03_29',\n",
       " '2020_03_30',\n",
       " '2020_03_31',\n",
       " '2020_04_01',\n",
       " '2020_04_02',\n",
       " '2020_04_03',\n",
       " '2020_04_04',\n",
       " '2020_04_05',\n",
       " '2020_04_06',\n",
       " '2020_04_07',\n",
       " '2020_04_08',\n",
       " '2020_04_09',\n",
       " '2020_04_10',\n",
       " '2020_04_11',\n",
       " '2020_04_12',\n",
       " '2020_04_13',\n",
       " '2020_04_14',\n",
       " '2020_04_15',\n",
       " '2020_04_16',\n",
       " '2020_04_17',\n",
       " '2020_04_18',\n",
       " '2020_04_19',\n",
       " '2020_04_20',\n",
       " '2020_04_21',\n",
       " '2020_04_22',\n",
       " '2020_04_23',\n",
       " '2020_04_24',\n",
       " '2020_04_25',\n",
       " '2020_04_26',\n",
       " '2020_04_27',\n",
       " '2020_04_28',\n",
       " '2020_04_29',\n",
       " '2020_04_30',\n",
       " '2020_05_01',\n",
       " '2020_05_02',\n",
       " '2020_05_03',\n",
       " '2020_05_04',\n",
       " '2020_05_05',\n",
       " '2020_05_06',\n",
       " '2020_05_07',\n",
       " '2020_05_08',\n",
       " '2020_05_09',\n",
       " '2020_05_10',\n",
       " '2020_05_11',\n",
       " '2020_05_12',\n",
       " '2020_05_13',\n",
       " '2020_05_14',\n",
       " '2020_05_15',\n",
       " '2020_05_16',\n",
       " '2020_05_17',\n",
       " '2020_05_18',\n",
       " '2020_05_21',\n",
       " '2020_05_22',\n",
       " '2020_05_23']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notinbq = sorted(list(set(storage_dates) - set(bq_tables)))\n",
    "notinbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob dailies/2020-03-22/2020-03-22_clean-dataset.json downloaded to /home/jupyter/covid_disinfo_detect/experiments/playground_data/2020-03-22_clean-dataset.json.\n",
      "Downloaded data from 2020-03-22..\n"
     ]
    }
   ],
   "source": [
    "for day in notinbq[:2]:\n",
    "    day = day.replace('_', '-')\n",
    "    utils.download_json(day)\n",
    "    print(f'Downloaded data from {day}..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(date, chunksize=50000):\n",
    "    cols_interest = [\n",
    "        'created_at',\n",
    "        'id_str',\n",
    "        'user',\n",
    "        'lang',\n",
    "        'full_text'\n",
    "    ]\n",
    "    \n",
    "    chunks = pd.read_json(\n",
    "        f'playground_data/{date}_clean-dataset.json',\n",
    "        lines=True,\n",
    "        chunksize=chunksize,\n",
    "        dtype={\n",
    "            'id_str': str,\n",
    "            'in_reply_to_status_id_str': str,\n",
    "            'quoted_status_id_str': str\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df = pd.concat(chunk for chunk in chunks)\n",
    "    print('Loaded data...\\n')\n",
    "    return df[cols_interest]\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df2clean = df.copy()\n",
    "    df2clean['user_id_str'] = df2clean['user'].apply(lambda user: str(user['id_str']))\n",
    "    df2clean.drop(labels = 'user', axis = 1, inplace = True)\n",
    "    dfclean = df2clean[['created_at', 'id_str', 'user_id_str', 'lang', 'full_text']]\n",
    "    print('Cleaned data...\\n')\n",
    "    return dfclean\n",
    "\n",
    "\n",
    "def data_setup(date):\n",
    "    df = clean_data(load_data(date))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_bigquery(df, date):\n",
    "    client = bigquery.Client(location='US')\n",
    "    dataset = client.get_dataset('twitter_dailies')\n",
    "    bq_date = date.replace('-', '_')    # need to slightly change date string\n",
    "    table_ref = dataset.table(bq_date)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, location='US')\n",
    "    job.result()\n",
    "    print(f'Loaded {job.output_rows} rows from dataframe to {table_ref.path}\\n')\n",
    "    \n",
    "    \n",
    "def data_bq_wrapper():\n",
    "    date = input('What is the date of the data that you would like to store in BigQuery?\\n')\n",
    "    df = data_setup(date)\n",
    "    load_bigquery(df, date)\n",
    "    print(f'Data from {date} successfully stored in BigQuery.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the date of the data that you would like to store in BigQuery?\n",
      " 2020-05-19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data...\n",
      "\n",
      "Cleaned data...\n",
      "\n",
      "Loaded 1009123 rows from dataframe to /projects/covid-disinfo-detect/datasets/twitter_dailies/tables/2020_05_19\n",
      "\n",
      "Data from 2020-05-19 successfully stored in BigQuery.\n",
      "\n",
      "CPU times: user 4min 13s, sys: 8.08 s, total: 4min 21s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_bq_wrapper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
