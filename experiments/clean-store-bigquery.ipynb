{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Clean & Store in BigQuery_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "from pandas_gbq.gbq import TableCreationError, NotFoundException\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _May 29, 2020_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day = '2020-05-24'\n",
    "#bucket_path = f'gs://thepanacealab_covid19twitter/dailies/{day}/{day}_clean-dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_gcs(bucket_path):\n",
    "    '''\n",
    "    Takes path to file in Google Cloud Storage Bucket\n",
    "    and returns a Pandas DataFrame\n",
    "    '''\n",
    "    df = pd.read_json(\n",
    "        bucket_path,\n",
    "        lines=True,\n",
    "        dtype={\n",
    "            'id_str': str,\n",
    "            'in_reply_to_status_id_str': str,\n",
    "            'quoted_status_id_str': str\n",
    "        }\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df = load_from_gcs(bucket_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df = utils.load_data('2020-03-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_parquet(df):\n",
    "    '''\n",
    "    Takes Panda DataFrame, cleans data into appropriate form for BigQuery.\n",
    "    '''\n",
    "    cols_of_interest = [\n",
    "        'created_at',\n",
    "        'id_str',\n",
    "        'user',\n",
    "        'lang',\n",
    "        'full_text'\n",
    "    ]\n",
    "    df2clean = df.loc[:, cols_of_interest]\n",
    "    df2clean['user_id_str'] = df2clean['user'].apply(lambda user: str(user['id_str']))\n",
    "    df2clean.drop(labels = 'user', axis = 1, inplace = True)\n",
    "    dfclean = df2clean[['created_at', 'id_str', 'user_id_str', 'lang', 'full_text']]\n",
    "    return dfclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#dfclean = clean_for_parquet(df)\n",
    "#assert len(dfclean) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfclean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_to_gcs(df, day):\n",
    "    df.to_parquet(\n",
    "        f'gs://thepanacealab_covid19twitter/dailies/{day}/{day}_tweets.parquet'\n",
    "    )\n",
    "    print('Dataframe uploaded to bucket as parquet file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_wrapper(day):\n",
    "    bucket_path = (\n",
    "        f'gs://thepanacealab_covid19twitter/dailies/'\n",
    "        + f'{day}/{day}_clean-dataset.json'\n",
    "    )\n",
    "    print(f'Loading data for {day}...')\n",
    "    df = load_from_gcs(bucket_path)\n",
    "    print(f'Cleaning data for {day}...')\n",
    "    dfclean = clean_for_parquet(df)\n",
    "    assert len(dfclean) == len(df)\n",
    "    print(f'Converting to parquet file & storing in {day} bucket.')\n",
    "    parquet_to_gcs(dfclean, day)\n",
    "    print(f'{day} successfully converted and stored in Storage.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day = '2020-05-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#data_prep_wrapper(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_json_dates(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix='dailies/')\n",
    "    json_files = [\n",
    "        str(i).split(',')[1].strip() for i in blobs \n",
    "        if str(i).split(',')[1].endswith('.json')\n",
    "    ]\n",
    "    json_dates = [i.split('/')[1] for i in json_files]\n",
    "    return json_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquet_dates(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix='dailies/')\n",
    "    parquet_files = [\n",
    "        str(i).split(',')[1].strip() for i in blobs \n",
    "        if str(i).split(',')[1].endswith('.parquet')\n",
    "    ]\n",
    "    parquet_dates = [\n",
    "        i.split('/')[1] for i in parquet_files\n",
    "    ]\n",
    "    return parquet_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-05-23', '2020-05-24', '2020-05-25']\n"
     ]
    }
   ],
   "source": [
    "json_dates = list_json_dates()\n",
    "parquet_dates = list_parquet_dates()\n",
    "print(parquet_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-22', '2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08', '2020-04-09', '2020-04-10', '2020-04-11', '2020-04-12', '2020-04-13', '2020-04-14', '2020-04-15', '2020-04-16', '2020-04-17', '2020-04-18', '2020-04-19', '2020-04-20', '2020-04-21', '2020-04-22', '2020-04-23', '2020-04-24', '2020-04-25', '2020-04-26', '2020-04-27', '2020-04-28', '2020-04-29', '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09', '2020-05-10', '2020-05-11', '2020-05-12', '2020-05-13', '2020-05-14', '2020-05-15', '2020-05-16', '2020-05-17', '2020-05-18', '2020-05-19', '2020-05-20', '2020-05-21', '2020-05-22', '2020-05-23', '2020-05-24', '2020-05-25', '2020-05-27']\n"
     ]
    }
   ],
   "source": [
    "print(json_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-22', '2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08', '2020-04-09', '2020-04-10', '2020-04-11', '2020-04-12', '2020-04-13', '2020-04-14', '2020-04-15', '2020-04-16', '2020-04-17', '2020-04-18', '2020-04-19', '2020-04-20', '2020-04-21', '2020-04-22', '2020-04-23', '2020-04-24', '2020-04-25', '2020-04-26', '2020-04-27', '2020-04-28', '2020-04-29', '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09', '2020-05-10', '2020-05-11', '2020-05-12', '2020-05-13', '2020-05-14', '2020-05-15', '2020-05-16', '2020-05-17', '2020-05-18', '2020-05-19', '2020-05-20', '2020-05-21', '2020-05-22', '2020-05-27']\n"
     ]
    }
   ],
   "source": [
    "need_parquet = sorted(list(set(json_dates) - set(parquet_dates)))\n",
    "print(need_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 2020-05-21...\n",
      "Cleaning data for 2020-05-21...\n",
      "Converting to parquet file & storing in 2020-05-21 bucket.\n",
      "Dataframe uploaded to bucket as parquet file.\n",
      "2020-05-21 successfully converted and stored in Storage.\n",
      "Loading data for 2020-05-22...\n",
      "Cleaning data for 2020-05-22...\n",
      "Converting to parquet file & storing in 2020-05-22 bucket.\n",
      "Dataframe uploaded to bucket as parquet file.\n",
      "2020-05-22 successfully converted and stored in Storage.\n",
      "Loading data for 2020-05-27...\n",
      "Cleaning data for 2020-05-27...\n",
      "Converting to parquet file & storing in 2020-05-27 bucket.\n",
      "Dataframe uploaded to bucket as parquet file.\n",
      "2020-05-27 successfully converted and stored in Storage.\n",
      "CPU times: user 18min, sys: 1min 24s, total: 19min 25s\n",
      "Wall time: 19min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# time with 3 most recent days\n",
    "for day in need_parquet[-3:]:\n",
    "    data_prep_wrapper(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bq_create_table(df, day):\n",
    "#    '''\n",
    "#    Takes Pandas DataFrame, and it's associated date, and stores in BigQuery table\n",
    "#   '''\n",
    "#   bigquery_client = bigquery.Client()\n",
    "#    dataset_ref = bigquery_client.dataset('twitter_dailies')\n",
    "#    # BigQuery only accepts underscores for table names\n",
    "#    bq_day = day.replace('-', '_')\n",
    "#    # Prepares a reference to the table\n",
    "#    table_ref = dataset_ref.table(bq_day)\n",
    "#\n",
    "#    try:\n",
    "#        found = bigquery_client.get_table(table_ref)\n",
    "#        return found\n",
    "#    except Exception as e:\n",
    "#        table_ref = dataset_ref.table(bq_day)\n",
    "#        job = bigquery_client.load_table_from_dataframe(df, table_ref, location='US')\n",
    "        # waits for table load to complete\n",
    "#        job.result()\n",
    "#        print(\n",
    "#            f'Loaded dataframe from {day} with {job.output_rows} observations into',\n",
    "#            + f' {table_ref.path}'\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_create_table(df, day):\n",
    "    '''\n",
    "    Takes Pandas DataFrame, and it's associated date, and stores in BigQuery table\n",
    "    '''\n",
    "    # BigQuery only accepts underscores for table names\n",
    "    bq_day = day.replace('-', '_')\n",
    "    # prepares reference to the table\n",
    "    try:\n",
    "        table_schema = [\n",
    "            {'name': 'created_at', 'type': 'TIMESTAMP'},\n",
    "            {'name': 'id_str', 'type': 'STRING'},\n",
    "            {'name': 'user_id_str', 'type': 'STRING'},\n",
    "            {'name': 'lang', 'type': 'STRING'},\n",
    "            {'name': 'full_text', 'type': 'STRING'}\n",
    "        ]\n",
    "        df.to_gbq(\n",
    "            project_id='covid-disinfo-detect',\n",
    "            destination_table=f'twitter_dailies.{bq_day}',\n",
    "            if_exists='fail'\n",
    "        )\n",
    "    except TableCreationError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def bq_create_table(df, day):\n",
    "#    '''\n",
    "#    Takes Pandas DataFrame, and it's associated date, and stores in BigQuery table\n",
    "#    '''\n",
    "#    client = bigquery.Client()\n",
    "#    dataset_ref = client.dataset('twitter_dailies')\n",
    "#    # BigQuery only accepts underscores for table names\n",
    "#    bq_day = day.replace('-', '_')\n",
    "#    # prepares reference to the table\n",
    "#    table_ref = dataset_ref.table(bq_day)\n",
    "#    try:\n",
    "#        found = client.get_table(table_ref)\n",
    "#        print(found)\n",
    "#    except NotFoundException as e:\n",
    "#        job_config = bigquery.LoadJobConfig(schema=[\n",
    "#            bigquery.SchemaField('created_at', 'TIMESTAMP'),\n",
    "#            bigquery.SchemaField('id_str', 'STRING'),\n",
    "#            bigquery.SchemaField('user_id_str', 'STRING'),\n",
    "#            bigquery.SchemaField('lang', 'STRING'),\n",
    "#            bigquery.SchemaField('full_text', 'STRING')\n",
    "#        ])\n",
    "#        job = client.load_table_from_dataframe(\n",
    "#            df, table_ref, job_config=job_config\n",
    "#        )\n",
    "#        job.result()\n",
    "#        print(\"Loaded dataframe to {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#bq_create_table(dfclean, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion_bigquery(day):\n",
    "    bucket_path = (\n",
    "        f'gs://thepanacealab_covid19twitter/dailies/{day}/'\n",
    "        + f'{day}_clean-dataset.json'\n",
    "    )\n",
    "    print(f'Loading data for {day}...')\n",
    "    df = load_from_gcs(bucket_path)\n",
    "    print(f'Loaded data for {day}...')\n",
    "    dfclean = clean_for_bigquery(df)\n",
    "    print(f'Cleaned data for BigQuery...')\n",
    "    assert len(dfclean) == len(df)\n",
    "    bq_create_table(dfclean, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '2020-05-25'\n",
    "#day_list = ['2020-05-25', '2020-05-24', '2020-05-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for day in day_list:\n",
    "#    print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for day in day_list:\n",
    "#    data_ingestion_bigquery(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 2020-05-25...\n",
      "Loaded data for 2020-05-25...\n",
      "Cleaned data for BigQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.10s/it]"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, dataframe, dataset_id, table_id, chunksize, schema, progress_bar)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mremaining_rows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 logger.info(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/load.py\u001b[0m in \u001b[0;36mload_chunks\u001b[0;34m(client, dataframe, dataset_id, table_id, chunksize, schema, location)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             ).result()\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f848ba319193>\u001b[0m in \u001b[0;36mdata_ingestion_bigquery\u001b[0;34m(day)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cleaned data for BigQuery...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfclean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbq_create_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-dbeb6be1c142>\u001b[0m in \u001b[0;36mbq_create_table\u001b[0;34m(df, day)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'covid-disinfo-detect'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mdestination_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'twitter_dailies.{bq_day}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fail'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTableCreationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(self, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials)\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m         )\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mprivate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mto_gbq\u001b[0;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, verbose, private_key)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, dataframe, dataset_id, table_id, chunksize, schema, progress_bar)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# <https://cloud.google.com/bigquery/troubleshooting-errors>`__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     def run_query(\n",
      "\u001b[0;31mGenericGBQException\u001b[0m: Reason: 400 Error while reading data, error message: CSV table encountered too many errors, giving up. Rows: 194510; errors: 1. Please look into the errors[] collection for more details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_ingestion_bigquery(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#utils.download_json('2020-05-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets in covid-disinfo-detect:\n",
      "\tcovid-disinfo-detect.appspot.com\n",
      "\tstaging.covid-disinfo-detect.appspot.com\n",
      "\tthepanacealab_covid19twitter\n"
     ]
    }
   ],
   "source": [
    "sb_client = storage.Client()\n",
    "buckets = sb_client.list_buckets()\n",
    "\n",
    "print(\"Buckets in {}:\".format(sb_client.project))\n",
    "for item in buckets:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "        \n",
    "start_dt = date(2015, 12, 20)\n",
    "end_dt = date(2016, 1, 11)\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    print(dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_exists(day, bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(f'dailies/{day}/{day}_clean-dataset.json')\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_bucket(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix='dailies/', suff))\n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_blobs_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_exists('2020-05-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_tables_list():\n",
    "    # gather list of tables in bigquery\n",
    "    client = bigquery.Client()\n",
    "    dataset = client.get_dataset('twitter_dailies')\n",
    "    tables = list(client.list_tables(dataset))\n",
    "    return [table.table_id for table in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_tables = bigquery_tables_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_bucket(bucket_name='thepanacealab_covid19twitter'):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix='dailies/')\n",
    "    json_files = [\n",
    "        str(i).split(',')[1].strip() for i in blobs if str(i).split(',')[1].endswith('.json')\n",
    "    ]\n",
    "    storage_dates = [i.split('/')[1].replace('-', '_') for i in json_files]\n",
    "    return json_files, storage_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files, storage_dates = list_blobs_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_05_19', '2020_05_20']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigquery_tables_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_03_22',\n",
       " '2020_03_23',\n",
       " '2020_03_24',\n",
       " '2020_03_25',\n",
       " '2020_03_26',\n",
       " '2020_03_27',\n",
       " '2020_03_28',\n",
       " '2020_03_29',\n",
       " '2020_03_30',\n",
       " '2020_03_31',\n",
       " '2020_04_01',\n",
       " '2020_04_02',\n",
       " '2020_04_03',\n",
       " '2020_04_04',\n",
       " '2020_04_05',\n",
       " '2020_04_06',\n",
       " '2020_04_07',\n",
       " '2020_04_08',\n",
       " '2020_04_09',\n",
       " '2020_04_10',\n",
       " '2020_04_11',\n",
       " '2020_04_12',\n",
       " '2020_04_13',\n",
       " '2020_04_14',\n",
       " '2020_04_15',\n",
       " '2020_04_16',\n",
       " '2020_04_17',\n",
       " '2020_04_18',\n",
       " '2020_04_19',\n",
       " '2020_04_20',\n",
       " '2020_04_21',\n",
       " '2020_04_22',\n",
       " '2020_04_23',\n",
       " '2020_04_24',\n",
       " '2020_04_25',\n",
       " '2020_04_26',\n",
       " '2020_04_27',\n",
       " '2020_04_28',\n",
       " '2020_04_29',\n",
       " '2020_04_30',\n",
       " '2020_05_01',\n",
       " '2020_05_02',\n",
       " '2020_05_03',\n",
       " '2020_05_04',\n",
       " '2020_05_05',\n",
       " '2020_05_06',\n",
       " '2020_05_07',\n",
       " '2020_05_08',\n",
       " '2020_05_09',\n",
       " '2020_05_10',\n",
       " '2020_05_11',\n",
       " '2020_05_12',\n",
       " '2020_05_13',\n",
       " '2020_05_14',\n",
       " '2020_05_15',\n",
       " '2020_05_16',\n",
       " '2020_05_17',\n",
       " '2020_05_18',\n",
       " '2020_05_19',\n",
       " '2020_05_20',\n",
       " '2020_05_21',\n",
       " '2020_05_22',\n",
       " '2020_05_23']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020_03_22',\n",
       " '2020_03_23',\n",
       " '2020_03_24',\n",
       " '2020_03_25',\n",
       " '2020_03_26',\n",
       " '2020_03_27',\n",
       " '2020_03_28',\n",
       " '2020_03_29',\n",
       " '2020_03_30',\n",
       " '2020_03_31',\n",
       " '2020_04_01',\n",
       " '2020_04_02',\n",
       " '2020_04_03',\n",
       " '2020_04_04',\n",
       " '2020_04_05',\n",
       " '2020_04_06',\n",
       " '2020_04_07',\n",
       " '2020_04_08',\n",
       " '2020_04_09',\n",
       " '2020_04_10',\n",
       " '2020_04_11',\n",
       " '2020_04_12',\n",
       " '2020_04_13',\n",
       " '2020_04_14',\n",
       " '2020_04_15',\n",
       " '2020_04_16',\n",
       " '2020_04_17',\n",
       " '2020_04_18',\n",
       " '2020_04_19',\n",
       " '2020_04_20',\n",
       " '2020_04_21',\n",
       " '2020_04_22',\n",
       " '2020_04_23',\n",
       " '2020_04_24',\n",
       " '2020_04_25',\n",
       " '2020_04_26',\n",
       " '2020_04_27',\n",
       " '2020_04_28',\n",
       " '2020_04_29',\n",
       " '2020_04_30',\n",
       " '2020_05_01',\n",
       " '2020_05_02',\n",
       " '2020_05_03',\n",
       " '2020_05_04',\n",
       " '2020_05_05',\n",
       " '2020_05_06',\n",
       " '2020_05_07',\n",
       " '2020_05_08',\n",
       " '2020_05_09',\n",
       " '2020_05_10',\n",
       " '2020_05_11',\n",
       " '2020_05_12',\n",
       " '2020_05_13',\n",
       " '2020_05_14',\n",
       " '2020_05_15',\n",
       " '2020_05_16',\n",
       " '2020_05_17',\n",
       " '2020_05_18',\n",
       " '2020_05_21',\n",
       " '2020_05_22',\n",
       " '2020_05_23']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notinbq = sorted(list(set(storage_dates) - set(bq_tables)))\n",
    "notinbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob dailies/2020-03-22/2020-03-22_clean-dataset.json downloaded to /home/jupyter/covid_disinfo_detect/experiments/playground_data/2020-03-22_clean-dataset.json.\n",
      "Downloaded data from 2020-03-22..\n"
     ]
    }
   ],
   "source": [
    "for day in notinbq[:2]:\n",
    "    day = day.replace('_', '-')\n",
    "    utils.download_json(day)\n",
    "    print(f'Downloaded data from {day}..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(date, chunksize=50000):\n",
    "    cols_interest = [\n",
    "        'created_at',\n",
    "        'id_str',\n",
    "        'user',\n",
    "        'lang',\n",
    "        'full_text'\n",
    "    ]\n",
    "    \n",
    "    chunks = pd.read_json(\n",
    "        f'playground_data/{date}_clean-dataset.json',\n",
    "        lines=True,\n",
    "        chunksize=chunksize,\n",
    "        dtype={\n",
    "            'id_str': str,\n",
    "            'in_reply_to_status_id_str': str,\n",
    "            'quoted_status_id_str': str\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df = pd.concat(chunk for chunk in chunks)\n",
    "    print('Loaded data...\\n')\n",
    "    return df[cols_interest]\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df2clean = df.copy()\n",
    "    df2clean['user_id_str'] = df2clean['user'].apply(lambda user: str(user['id_str']))\n",
    "    df2clean.drop(labels = 'user', axis = 1, inplace = True)\n",
    "    dfclean = df2clean[['created_at', 'id_str', 'user_id_str', 'lang', 'full_text']]\n",
    "    print('Cleaned data...\\n')\n",
    "    return dfclean\n",
    "\n",
    "\n",
    "def data_setup(date):\n",
    "    df = clean_data(load_data(date))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_bigquery(df, date):\n",
    "    client = bigquery.Client(location='US')\n",
    "    dataset = client.get_dataset('twitter_dailies')\n",
    "    bq_date = date.replace('-', '_')    # need to slightly change date string\n",
    "    table_ref = dataset.table(bq_date)\n",
    "    job = client.load_table_from_dataframe(df, table_ref, location='US')\n",
    "    job.result()\n",
    "    print(f'Loaded {job.output_rows} rows from dataframe to {table_ref.path}\\n')\n",
    "    \n",
    "    \n",
    "def data_bq_wrapper():\n",
    "    date = input('What is the date of the data that you would like to store in BigQuery?\\n')\n",
    "    df = data_setup(date)\n",
    "    load_bigquery(df, date)\n",
    "    print(f'Data from {date} successfully stored in BigQuery.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the date of the data that you would like to store in BigQuery?\n",
      " 2020-05-19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data...\n",
      "\n",
      "Cleaned data...\n",
      "\n",
      "Loaded 1009123 rows from dataframe to /projects/covid-disinfo-detect/datasets/twitter_dailies/tables/2020_05_19\n",
      "\n",
      "Data from 2020-05-19 successfully stored in BigQuery.\n",
      "\n",
      "CPU times: user 4min 13s, sys: 8.08 s, total: 4min 21s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_bq_wrapper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
