{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Getting the Text Pre-Processed_\n",
    "\n",
    "So we can gather live Twitter data, but as I noted in a previous notebook, we need to take the text we have and clean it up, which means we'll have to do some text pre-processing.\n",
    "\n",
    "To make things a little easier, and so that we don't run into any issues with the Twitter API (i.e. we don't make too many necessary calls to it), we'll be using data that's already been acquired. This data has tweets ranging from March 20, 2020 at ~1:30am through March 24, 2020 at 11:59pm. In this case, historical data gives us a good representation of what the streaming data will look like, and to experiment with various text pre-processing strategies without worrying about mistakenly eliminating too much information. \n",
    "\n",
    "The tools we'll primarily be working with will be regular expressions and the [`re`](https://docs.python.org/2/library/re.html) library, which provides operations for regular expression matching in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n",
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import fundamentals\n",
    "pd.options.display.max_columns = None\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.simplefilter(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Load Data_\n",
    "\n",
    "We'll start by loading in the data. The pickle file we have - `covid19_0320_0324.pkl` - contains roughly 3.3M Tweets from March 20th through March 24th. We'll load it into a pandas `DataFrame`, which will allow us to then start experimenting with text preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      "dtypes: bool(1), int64(7), object(11)\n",
      "memory usage: 490.9+ MB\n",
      "CPU times: user 4.87 s, sys: 2.57 s, total: 7.44 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# strings of file paths and file name for data\n",
    "origpath = \"/notebooks/CovidDisinfo-Detect/experiments\"\n",
    "datapath = \"/notebooks/CovidDisinfo-Detect/data/interim\"\n",
    "filename1 = \"covid19_0320_0324.pkl\"\n",
    "\n",
    "# load data into pandas dataframe\n",
    "df = fundamentals.load_data(origpath, datapath, filename1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Remove Newline Characters_\n",
    "\n",
    "The first step will be to remove newline characters, which appear in the text as `\\n`. Additionally there are plenty of instances where there is more than one of these characters, and even cases where there are two back-to-back. We need to be able to remove all of these instances, and the `re` library can help us do just that. \n",
    "\n",
    "The first function I'm going to define - `newline_seach` - will allow us to gain a better idea of how newline characters work within the text, and more importantly, to see how frequently they pop up within text.\n",
    "\n",
    "Next we'll define a function that takes in a given text, and will sub every instance of `\\n` for a simple space. Additionally, when we run this function on the DataFrame, we'll create a new column - `processed_text` - to indicate that this column is our pre-processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newline_search(text):\n",
    "    # re.I means it is not case-sensitive\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return \",\".join(x.group() for x in regex.finditer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2020-03-24 23:59:59                \\n,\\n,\\n\n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                   \\n,\\n\n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                      \\n\n",
       "2020-03-24 23:59:58    \\n,\\n,\\n,\\n,\\n,\\n,\\n\n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                      \\n\n",
       "2020-03-24 23:59:57                        \n",
       "2020-03-24 23:59:57                        \n",
       "2020-03-24 23:59:57                 \\n\\n,\\n\n",
       "2020-03-24 23:59:57                        \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment on first 20 observations of DataFrame\n",
    "df[\"tweet\"][:20].apply(newline_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not every observation has them, we can see that they occur fairly frequently and we can see at the bottom an example of a back-to-back newline character (i.e. `\\n\\n`). What else this showcases is how powerful the `re` library can be for capturing particular patterns in a text. Given a relatively simple raw string - `r\"\\n+` - it is able to capture the various instances of a newline character within a given text.\n",
    "\n",
    "Now that we have an idea of what we're replacing, we can move to replacing them with a `newline_remove` function and subsequently reating our new `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newline_remove(text):\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return regex.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we first apply the newline_remove function and then apply newline_search to see if there are any \\n's\n",
    "df[\"tweet\"][:20].apply(newline_remove).apply(newline_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is just an example (and a double check) to make sure our `newline_remove` works as intended. Now we'll use it to create our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 379 ms, total: 19.3 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"processed_tweet\"] = df[\"tweet\"].apply(newline_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace Twitter picture, YouTube and other URLs with \"fillers\"_\n",
    "\n",
    "Since we are working with Twitter data, we need to address potential components of a Tweet (outside of its text), namely URLs. Each tweet could contains links to such things as a picture, a YouTube video, or a news article. The first one we'll focus on are **Twitter pictures**, which are in the following format:\n",
    "- `pic.twitter.com/(random assortment of numbers & letters)`\n",
    "\n",
    "We'll replace the above link with `pictwitter` for two reasons: it allows us to acknowledge that there was a picture in a given tweet, and it'll make it less likely that any model we produce can key in on particular url. In the following cells, we'll define the function `twitterpic_replace`, then show a quick example of what it does, and then apply it to the `processed_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"xxpictwit\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many in our community that are elderly, have compromised immune symptoms or are otherwise high risk for COVID-19. Print and Hang this sign on your door to help protect yourself from the public.  xxpictwit\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! #jasonkenney #COVID19 #N95 #abhealth xxpictwit\n",
      "\n",
      "Wana come up to my Hotel Room? . . . . . I have a Big Hand Sanitizer & Two LOO Rolls ðŸ’©ðŸ’© #COVIDIDIOTS #covid19UK #CoronavirusLockdown #COVID19  xxpictwit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# an example of what our twitter_replace function does, note the end of each of the texts below\n",
    "for n in range(3):\n",
    "    print(df[df[\"photos\"] != \"none\"][\"processed_tweet\"][n:n+1].apply(twitterpic_replace).iloc[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c945303edf4fbf8b03cba42c3ddcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to processed_tweet\n",
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(twitterpic_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've replaced the Twitter picture links, we'll turn our attention to **YouTube links**, following along with the method we established above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"xxyoutubeurl\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of what `youtube_replace` does. If you look at the end of the text, there is the word `youtubelink` which has been substituted for the YouTube URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican President Takes No National Safety Measures Against COVID-19  xxyoutubeurlÂ â€¦\n",
      "\n",
      "Koreans think Malaysia handles this covid-19 situation despite political instability for a while back there, better than Korean government and I live for these comments <3 @MuhyiddinYassin xxyoutubeurlÂ â€¦\n",
      "\n",
      "Daily COVID-19 FAQ: Becki Young explains the recent announcement from the DOL that, starting March 25th, all PERM approvals will now be issued electronically, though the originals with wet signatures are still required upon submission to USCIS.  xxyoutubeurlÂ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print((df[df[\"processed_tweet\"].str.contains(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)]\n",
    "     [\"processed_tweet\"][n:n+1].apply(youtube_replace).iloc[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b370ed38b30a4ba496ecaa88603b5ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(youtube_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll see how we can go about replacing **all remaining URLs** in the Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"xxurl\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the links are going to vary a lot more than the two previous examples because we're searching for all other links that aren't tied to either Twitter or YouTube. As a test, I'm going to extract 5 random samples from the `processed_tweet` column, then print what the text looked like pre and post application of the `url_replace` function. Once we're sure it's picking up all other URL links, than we can go ahead an apply it to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (df[df[\"processed_tweet\"].str.contains(r\"(?:http|ftp|https)://(\\S+)\", re.I)])[\"processed_tweet\"].sample(n=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll print out the 5 observations to see what they look like before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSD students are staying connected in a time of #socialdistancing during #COVID19 #schoolclosures with help from technology and their fun-loving, creative principals + teachers. Find out how on #CUSDInsider! http://cusdinsider.org/capistrano-unified-principals-engage-students-virtually-during-covid-19-closures/Â â€¦\n",
      "\n",
      " http://BlackburnNews.comÂ : Huron Country creates resource list for businesses affected by COVID-19.  https://blackburnnews.com/midwestern-ontario/midwestern-ontario-news/2020/03/20/huron-country-creates-resource-list-businesses-affected-covid-19/Â â€¦ via @GoogleNews\n",
      "\n",
      "#HarveyWeinstein Contracts #Coronavirus While in Custody, According to Reports  https://bit.ly/2xgqwiXÂ  #COVID19 #NewYork xxpictwit\n",
      "\n",
      "This research found that 1 in 7 patients hospitalized with Covid-19 has acquired a dangerous secondary bacterial infection, and 50% of patients who have died had such infections.  https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30566-3/fulltext#tbl2Â â€¦\n",
      "\n",
      "They may have thought that, this is the way one can can rid of #Covid_19 ðŸ˜¬  https://twitter.com/tabassum_b/status/1241805031814230021Â â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test)):\n",
    "    print(test[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what they look like after..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSD students are staying connected in a time of #socialdistancing during #COVID19 #schoolclosures with help from technology and their fun-loving, creative principals + teachers. Find out how on #CUSDInsider! xxurlÂ â€¦\n",
      "\n",
      " xxurlÂ : Huron Country creates resource list for businesses affected by COVID-19.  xxurlÂ â€¦ via @GoogleNews\n",
      "\n",
      "#HarveyWeinstein Contracts #Coronavirus While in Custody, According to Reports  xxurlÂ  #COVID19 #NewYork xxpictwit\n",
      "\n",
      "This research found that 1 in 7 patients hospitalized with Covid-19 has acquired a dangerous secondary bacterial infection, and 50% of patients who have died had such infections.  xxurlÂ â€¦\n",
      "\n",
      "They may have thought that, this is the way one can can rid of #Covid_19 ðŸ˜¬  xxurlÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test)):\n",
    "    print(url_replace(test[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It captured all the links from above, so let's go ahead and apply this to the entire `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1c47e41ffb476596aa6de9179a2f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(url_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace User Mentions & Hashtags with fillers_\n",
    "\n",
    "Now the last two components of the text we'll have to address are user mentions and hashtags. A user mention comprises an `@` symbol plus another user's name, while a hashtag is usually comprised of a `#` with a word directly behind it (sometimes, they can be comprised of multiple words put together). Below is an example of a tweet containing a user mention and a hashtag:\n",
    "- `Hey @earny_joe, how is your project coming along? #DataScience`\n",
    "\n",
    "In the text, `@earny_joe` represents a user mention and `#DataScience` is a hashtag. For the first version of this pipeline, which constitues a baseline of sorts, I'm going to replace both user mentions and hashtags with a filler, similar to what we did with the links above. However, I want to point out, that this information, particularly in the case of hashtags, could provide value for future iterations of the pipeline/model. \n",
    "\n",
    "That being said lets go ahead and define functions that address these two components and apply them to the `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usermention_replace(text):\n",
    "    regex = re.compile(r\"@([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxuser\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab 5 observations to showcase usermention_replace\n",
    "test5 = df[df[\"processed_tweet\"].str.contains(r\"@([^\\s:]+)+\", re.I)][\"processed_tweet\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before `usermention_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19: Airlines And OTAs Say, â€˜Donâ€™t Call Usâ€™ As Travel Agents Come To The Rescue via @forbes xxurlÂ â€¦\n",
      "\n",
      "Help make it happen for C3 Test for Coronavirus & COVID-19 on @indiegogo xxurlÂ \n",
      "\n",
      "@drgregpoland I sent you a LinkedIn request. Thanks for your timely information on Covid 19. I admire your work. I admire Mayo Clinic. My wife Mary Ann Edwards is a pharmacist\n",
      "\n",
      "So, not only was the order screwed up, and billing screwed up, The @att store at the Valley Mall robbed us. And acccording to 611 there is shit we can do about it because they canâ€™t contact the store until the #COVID19 pandemic is over.\n",
      "\n",
      "So you got your #slushfund and now you want people to go to work? You do not understand the risk of COVID19! @realDonaldTrump xxurlÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(test5[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After `usermention_reaplce`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19: Airlines And OTAs Say, â€˜Donâ€™t Call Usâ€™ As Travel Agents Come To The Rescue via xxuser xxurlÂ â€¦\n",
      "\n",
      "Help make it happen for C3 Test for Coronavirus & COVID-19 on xxuser xxurlÂ \n",
      "\n",
      "xxuser I sent you a LinkedIn request. Thanks for your timely information on Covid 19. I admire your work. I admire Mayo Clinic. My wife Mary Ann Edwards is a pharmacist\n",
      "\n",
      "So, not only was the order screwed up, and billing screwed up, The xxuser store at the Valley Mall robbed us. And acccording to 611 there is shit we can do about it because they canâ€™t contact the store until the #COVID19 pandemic is over.\n",
      "\n",
      "So you got your #slushfund and now you want people to go to work? You do not understand the risk of COVID19! xxuser xxurlÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(usermention_replace(test5[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2095ee83c8cc46259d093e15145dfba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(usermention_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll turn our attention to **replacing the hashtags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_replace(text):\n",
    "    regex = re.compile(r\"#([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxhashtag\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123 = \"Hey @earny_joe, how are you doing during the #coronavirus #COVID_19 pandemic? #StayHome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey @earny_joe, how are you doing during the xxhashtag xxhashtag pandemic? xxhashtag'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_replace(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab 5 observations to showcase hashtag_replace\n",
    "test5 = df[df[\"processed_tweet\"].str.contains(r\"#([^\\s:]+)+\", re.I)][\"processed_tweet\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before `hashtag_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got fired yesterday due to the damage of coronavirus. Not only me, 70 colleagues got fired from my company. People are dying. Companies are closing. CHINA MUST TAKE RESPONSIBILITY FOR THIS!!!!!!! #COVID19 #ChinaVirus\n",
      "\n",
      "NEW #CORONAVIRUS #COVID19 #VETgirl webinar - FREE - starting in 30 minutes, 8:30pm Eastern.   Sign up now!  Get the info YOU need to keep yourself, your practice, your clients, and your patients safe!  #Veterinary #VetMed #VetTech #Pandemic xxurlÂ â€¦\n",
      "\n",
      "You may be hearing a lot of concerning information in the news, and even in these #coronavirus prevention tip videos. But the best way to proceed is to stay calm, stay informed, and know the facts. Learn more here. #COVID19  xxurlÂ \n",
      "\n",
      "honestly? This shit is dark. What the Texas lieutenant governor proposed IN REAL LIFE is what I proposed as an absurd, way over-the-top (because it's so overtly inhumane!) remedy meant to call attention to a real problem  xxurlÂ â€¦ h/t Swift #CoronavirusUSA #COVID19\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! #jasonkenney #COVID19 #N95 #abhealth xxpictwit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(test5[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After `hashtag_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got fired yesterday due to the damage of coronavirus. Not only me, 70 colleagues got fired from my company. People are dying. Companies are closing. CHINA MUST TAKE RESPONSIBILITY FOR THIS!!!!!!! xxhashtag xxhashtag\n",
      "\n",
      "NEW xxhashtag xxhashtag xxhashtag webinar - FREE - starting in 30 minutes, 8:30pm Eastern.   Sign up now!  Get the info YOU need to keep yourself, your practice, your clients, and your patients safe!  xxhashtag xxhashtag xxhashtag xxhashtag xxurlÂ â€¦\n",
      "\n",
      "You may be hearing a lot of concerning information in the news, and even in these xxhashtag prevention tip videos. But the best way to proceed is to stay calm, stay informed, and know the facts. Learn more here. xxhashtag  xxurlÂ \n",
      "\n",
      "honestly? This shit is dark. What the Texas lieutenant governor proposed IN REAL LIFE is what I proposed as an absurd, way over-the-top (because it's so overtly inhumane!) remedy meant to call attention to a real problem  xxurlÂ â€¦ h/t Swift xxhashtag xxhashtag\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! xxhashtag xxhashtag xxhashtag xxhashtag xxpictwit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(hashtag_replace(test5[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d97fb9849848dcb0e77d92bf2ff0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(hashtag_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to stop here for today (**reference: April 7, 2020 @ 4:19pm PDT**). In the following cell, I'm going to save the dataframe that I have so far into a pickle file - `covid19_0320_0324_updated.pkl` and store in the `playground_data` folder, which will allow me to store data that I've edited but that isn't quite in its final form yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle(\"playground_data/covid19_0320_0324_updated.pkl\")\n",
    "#df = pd.read_pickle(\"playground_data/covid19_0320_0324_updated.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Emojis_\n",
    "\n",
    "The last component, and potentially most interesting, of a Tweet is the emoji. Today, emoji's are no longer simple smiley faces, but a wide range of images from thumbs up, to check marks, to the flags of countries. Luckily, there is a Python library with the creative name of [`emoji`](https://github.com/carpedm20/emoji/) to help us address this. \n",
    "\n",
    "It gives us the ability to `demojize` text. For example, let's say we have the text: `Python is ðŸ‘`. All we need to do with the `emoji` library is the following:\n",
    "\n",
    "- `emoji.demojize(\"Python is ðŸ‘\")\n",
    "\n",
    "This will be returned in the following form: `Python is :thumbs_up:`. All of the emojis are demojized into this form, so we can then use the `re` library to search for this particular pattern (e.g. `:emoji_name:`) and replace with a filler token, `xxemoji`. \n",
    "\n",
    "That being said, we'll take the following steps over the next few cells:\n",
    "- Develop an `emoji_replace` function that searches for this `:emoji_name:` pattern, and replaces with ` xxemoji ` (the additional space separate it from any other text that may be directly next to it)\n",
    "- Test out the above function on a small sample (text before & after)\n",
    "- Then apply the function to our pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# function to help grab indexes for data that we can test below function on \n",
    "def emoji_detector(text):\n",
    "    regex = re.compile(r\"(:\\S+:)+\", re.I)\n",
    "    return \",\".join(x.group() for x in regex.finditer(text))\n",
    "\n",
    "def emoji_replace(text):\n",
    "    # first demojize text\n",
    "    new_text = emoji.demojize(text, use_aliases=True)\n",
    "    regex = re.compile(r\"(:\\S+:)+\", re.I)\n",
    "    return regex.sub(\" xxemoji \", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e454f722ebc54137823d718eb9082b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd08a3ead17549919e8955d31a3ab8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# indexes from first 1000 observations that contain emoji\n",
    "emoji_idx = df[\"processed_tweet\"][:1000].progress_apply(lambda x: emoji.demojize(x)).progress_apply(emoji_detector) != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df where all text has emojis\n",
    "subset = df[:1000]; df_emojis = subset[emoji_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before `emoji_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! xxhashtag xxhashtag xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "Wana come up to my Hotel Room? . . . . . I have a Big Hand Sanitizer & Two LOO Rolls ðŸ’©ðŸ’© xxhashtag xxhashtag xxhashtag xxhashtag  xxpictwit\n",
      "\n",
      "Director xxuser talking about how our agency is adapting to challenges of COVID-19 and where to go to get the resources you need. For food, cash assistance or Medicaid ðŸ‘‡  xxurlÂ   For unemployment ðŸ‘‡  xxurlÂ  xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "xxhashtag âœ… Imagine $6 trillion and how it would go to people, not a coup for corporations.  âœ…Imagine having healthcare for you and enough medical personnel and equipment to deal with COVID19 âœ…Imagine being able to pay for your basic needs xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "Trump wants to lift all lockdowns throughout the nation by Easter and he said more people die from seasonal flu or car accidents than they do from COVID-19..... lol yâ€™all... I canâ€™t even explain how much I hate this oompa loompa looking ass fucktard ðŸ¥´ His presidency is a joke.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(df_emojis[\"processed_tweet\"][n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After `emoji_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was this photo even taken during the COVID crisis??  xxemoji  JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! xxhashtag xxhashtag xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "Wana come up to my Hotel Room? . . . . . I have a Big Hand Sanitizer & Two LOO Rolls  xxemoji  xxhashtag xxhashtag xxhashtag xxhashtag  xxpictwit\n",
      "\n",
      "Director xxuser talking about how our agency is adapting to challenges of COVID-19 and where to go to get the resources you need. For food, cash assistance or Medicaid  xxemoji   xxurlÂ   For unemployment  xxemoji   xxurlÂ  xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "xxhashtag  xxemoji  Imagine $6 trillion and how it would go to people, not a coup for corporations.   xxemoji Imagine having healthcare for you and enough medical personnel and equipment to deal with COVID19  xxemoji Imagine being able to pay for your basic needs xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "Trump wants to lift all lockdowns throughout the nation by Easter and he said more people die from seasonal flu or car accidents than they do from COVID-19..... lol yâ€™all... I canâ€™t even explain how much I hate this oompa loompa looking ass fucktard  xxemoji  His presidency is a joke.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(emoji_replace(df_emojis[\"processed_tweet\"][n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8553a71a684341118ffc268a5769ab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(emoji_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe so far for ease of future access\n",
    "df.to_pickle(\"playground_data/covid19_0320_0324_updated_v2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
