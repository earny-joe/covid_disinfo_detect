{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Getting the Text Pre-Processed_\n",
    "\n",
    "So we can gather live Twitter data, but as I noted in a previous notebook, we need to take the text we have and clean it up, which means we'll have to do some text pre-processing.\n",
    "\n",
    "To make things a little easier, and so that we don't run into any issues with the Twitter API (i.e. we don't make too many necessary calls to it), we'll be using data that's already been acquired. This data has tweets ranging from March 20, 2020 at ~1:30am through March 24, 2020 at 11:59pm. In this case, historical data gives us a good representation of what the streaming data will look like, and to experiment with various text pre-processing strategies without worrying about mistakenly eliminating too much information. \n",
    "\n",
    "The tools we'll primarily be working with will be regular expressions and the [`re`](https://docs.python.org/2/library/re.html) library, which provides operations for regular expression matching in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import fundamentals\n",
    "pd.options.display.max_columns = None\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.simplefilter(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Load Data_\n",
    "\n",
    "We'll start by loading in the data. The pickle file we have - `covid19_0320_0324.pkl` - contains roughly 3.3M Tweets from March 20th through March 24th. We'll load it into a pandas `DataFrame`, which will allow us to then start experimenting with text preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      "dtypes: bool(1), int64(7), object(11)\n",
      "memory usage: 490.9+ MB\n",
      "CPU times: user 5.48 s, sys: 2.74 s, total: 8.22 s\n",
      "Wall time: 8.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# strings of file paths and file name for data\n",
    "origpath = \"/notebooks/CovidDisinfo-Detect/experiments\"\n",
    "datapath = \"/notebooks/CovidDisinfo-Detect/data/interim\"\n",
    "filename1 = \"covid19_0320_0324.pkl\"\n",
    "\n",
    "# load data into pandas dataframe\n",
    "df = fundamentals.load_data(origpath, datapath, filename1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Remove Newline Characters_\n",
    "\n",
    "The first step will be to remove newline characters, which appear in the text as `\\n`. Additionally there are plenty of instances where there is more than one of these characters, and even cases where there are two back-to-back. We need to be able to remove all of these instances, and the `re` library can help us do just that. \n",
    "\n",
    "The first function I'm going to define - `newline_seach` - will allow us to gain a better idea of how newline characters work within the text, and more importantly, to see how frequently they pop up within text.\n",
    "\n",
    "Next we'll define a function that takes in a given text, and will sub every instance of `\\n` for a simple space. Additionally, when we run this function on the DataFrame, we'll create a new column - `processed_text` - to indicate that this column is our pre-processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newline_search(text):\n",
    "    # re.I means it is not case-sensitive\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return \",\".join(x.group() for x in regex.finditer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2020-03-24 23:59:59                \\n,\\n,\\n\n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                   \\n,\\n\n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:59                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                      \\n\n",
       "2020-03-24 23:59:58    \\n,\\n,\\n,\\n,\\n,\\n,\\n\n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                        \n",
       "2020-03-24 23:59:58                      \\n\n",
       "2020-03-24 23:59:57                        \n",
       "2020-03-24 23:59:57                        \n",
       "2020-03-24 23:59:57                 \\n\\n,\\n\n",
       "2020-03-24 23:59:57                        \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment on first 20 observations of DataFrame\n",
    "df[\"tweet\"][:20].apply(newline_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not every observation has them, we can see that they occur fairly frequently and we can see at the bottom an example of a back-to-back newline character (i.e. `\\n\\n`). What else this showcases is how powerful the `re` library can be for capturing particular patterns in a text. Given a relatively simple raw string - `r\"\\n+` - it is able to capture the various instances of a newline character within a given text.\n",
    "\n",
    "Now that we have an idea of what we're replacing, we can move to replacing them with a `newline_remove` function and subsequently reating our new `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newline_remove(text):\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return regex.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:59    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:58    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "2020-03-24 23:59:57    \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we first apply the newline_remove function and then apply newline_search to see if there are any \\n's\n",
    "df[\"tweet\"][:20].apply(newline_remove).apply(newline_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is just an example (and a double check) to make sure our `newline_remove` works as intended. Now we'll use it to create our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3e889cb984ebdbc0593bfc9ca89d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"tweet\"].progress_apply(newline_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace Twitter picture, YouTube and other URLs with \"fillers\"_\n",
    "\n",
    "Since we are working with Twitter data, we need to address potential components of a Tweet (outside of its text), namely URLs. Each tweet could contains links to such things as a picture, a YouTube video, or a news article. The first one we'll focus on are **Twitter pictures**, which are in the following format:\n",
    "- `pic.twitter.com/(random assortment of numbers & letters)`\n",
    "\n",
    "We'll replace the above link with `pictwitter` for two reasons: it allows us to acknowledge that there was a picture in a given tweet, and it'll make it less likely that any model we produce can key in on particular url. In the following cells, we'll define the function `twitterpic_replace`, then show a quick example of what it does, and then apply it to the `processed_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"pictwitter\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many in our community that are elderly, have compromised immune symptoms or are otherwise high risk for COVID-19. Print and Hang this sign on your door to help protect yourself from the public.  pictwitter\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! #jasonkenney #COVID19 #N95 #abhealth pictwitter\n",
      "\n",
      "Wana come up to my Hotel Room? . . . . . I have a Big Hand Sanitizer & Two LOO Rolls ðŸ’©ðŸ’© #COVIDIDIOTS #covid19UK #CoronavirusLockdown #COVID19  pictwitter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# an example of what our twitter_replace function does, note the end of each of the texts below\n",
    "for n in range(3):\n",
    "    print(df[df[\"photos\"] != \"none\"][\"processed_tweet\"][n:n+1].apply(twitterpic_replace).iloc[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e08f37f16d4e988e9d1c55009c7c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(twitterpic_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've replaced the Twitter picture links, we'll turn our attention to **YouTube links**, following along with the method we established above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"youtubelink\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of what `youtube_replace` does. If you look at the end of the text, there is the word `youtubelink` which has been substituted for the YouTube URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican President Takes No National Safety Measures Against COVID-19  youtubelinkÂ â€¦\n",
      "\n",
      "Koreans think Malaysia handles this covid-19 situation despite political instability for a while back there, better than Korean government and I live for these comments <3 @MuhyiddinYassin youtubelinkÂ â€¦\n",
      "\n",
      "Daily COVID-19 FAQ: Becki Young explains the recent announcement from the DOL that, starting March 25th, all PERM approvals will now be issued electronically, though the originals with wet signatures are still required upon submission to USCIS.  youtubelinkÂ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print((df[df[\"processed_tweet\"].str.contains(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)]\n",
    "     [\"processed_tweet\"][n:n+1].apply(youtube_replace).iloc[0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef49afaea144c9891568f0787e39dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(youtube_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll see how we can go about replacing **all remaining URLs** in the Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"urllink\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the links are going to vary a lot more than the two previous examples because we're searching for all other links that aren't tied to either Twitter or YouTube. As a test, I'm going to extract 5 random samples from the `processed_tweet` column, then print what the text looked like pre and post application of the `url_replace` function. Once we're sure it's picking up all other URL links, than we can go ahead an apply it to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (df[df[\"processed_tweet\"].str.contains(r\"(?:http|ftp|https)://(\\S+)\", re.I)])[\"processed_tweet\"].sample(n=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll print out the 5 observations to see what they look like before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSD students are staying connected in a time of #socialdistancing during #COVID19 #schoolclosures with help from technology and their fun-loving, creative principals + teachers. Find out how on #CUSDInsider! http://cusdinsider.org/capistrano-unified-principals-engage-students-virtually-during-covid-19-closures/Â â€¦\n",
      "\n",
      " http://BlackburnNews.comÂ : Huron Country creates resource list for businesses affected by COVID-19.  https://blackburnnews.com/midwestern-ontario/midwestern-ontario-news/2020/03/20/huron-country-creates-resource-list-businesses-affected-covid-19/Â â€¦ via @GoogleNews\n",
      "\n",
      "#HarveyWeinstein Contracts #Coronavirus While in Custody, According to Reports  https://bit.ly/2xgqwiXÂ  #COVID19 #NewYork pictwitter\n",
      "\n",
      "This research found that 1 in 7 patients hospitalized with Covid-19 has acquired a dangerous secondary bacterial infection, and 50% of patients who have died had such infections.  https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30566-3/fulltext#tbl2Â â€¦\n",
      "\n",
      "They may have thought that, this is the way one can can rid of #Covid_19 ðŸ˜¬  https://twitter.com/tabassum_b/status/1241805031814230021Â â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test)):\n",
    "    print(test[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what they look like after..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSD students are staying connected in a time of #socialdistancing during #COVID19 #schoolclosures with help from technology and their fun-loving, creative principals + teachers. Find out how on #CUSDInsider! urllinkÂ â€¦\n",
      "\n",
      " urllinkÂ : Huron Country creates resource list for businesses affected by COVID-19.  urllinkÂ â€¦ via @GoogleNews\n",
      "\n",
      "#HarveyWeinstein Contracts #Coronavirus While in Custody, According to Reports  urllinkÂ  #COVID19 #NewYork pictwitter\n",
      "\n",
      "This research found that 1 in 7 patients hospitalized with Covid-19 has acquired a dangerous secondary bacterial infection, and 50% of patients who have died had such infections.  urllinkÂ â€¦\n",
      "\n",
      "They may have thought that, this is the way one can can rid of #Covid_19 ðŸ˜¬  urllinkÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test)):\n",
    "    print(url_replace(test[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It captured all the links from above, so let's go ahead and apply this to the entire `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4f586c87d643eda30b98d3cf5a344b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(url_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Replace User Mentions & Hashtags with fillers_\n",
    "\n",
    "Now the last two components of the text we'll have to address are user mentions and hashtags. A user mention comprises an `@` symbol plus another user's name, while a hashtag is usually comprised of a `#` with a word directly behind it (sometimes, they can be comprised of multiple words put together). Below is an example of a tweet containing a user mention and a hashtag:\n",
    "- `Hey @earny_joe, how is your project coming along? #DataScience`\n",
    "\n",
    "In the text, `@earny_joe` represents a user mention and `#DataScience` is a hashtag. For the first version of this pipeline, which constitues a baseline of sorts, I'm going to replace both user mentions and hashtags with a filler, similar to what we did with the links above. However, I want to point out, that this information, particularly in the case of hashtags, could provide value for future iterations of the pipeline/model. \n",
    "\n",
    "That being said lets go ahead and define functions that address these two components and apply them to the `processed_tweet` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usermention_replace(text):\n",
    "    regex = re.compile(r\"@([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"usermention\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab 5 observations to showcase usermention_replace\n",
    "test5 = df[df[\"processed_tweet\"].str.contains(r\"@([^\\s:]+)+\", re.I)][\"processed_tweet\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before `usermention_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19: Airlines And OTAs Say, â€˜Donâ€™t Call Usâ€™ As Travel Agents Come To The Rescue via @forbes urllinkÂ â€¦\n",
      "\n",
      "Help make it happen for C3 Test for Coronavirus & COVID-19 on @indiegogo urllinkÂ \n",
      "\n",
      "@drgregpoland I sent you a LinkedIn request. Thanks for your timely information on Covid 19. I admire your work. I admire Mayo Clinic. My wife Mary Ann Edwards is a pharmacist\n",
      "\n",
      "So, not only was the order screwed up, and billing screwed up, The @att store at the Valley Mall robbed us. And acccording to 611 there is shit we can do about it because they canâ€™t contact the store until the #COVID19 pandemic is over.\n",
      "\n",
      "So you got your #slushfund and now you want people to go to work? You do not understand the risk of COVID19! @realDonaldTrump urllinkÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(test5[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After `usermention_reaplce`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19: Airlines And OTAs Say, â€˜Donâ€™t Call Usâ€™ As Travel Agents Come To The Rescue via usermention urllinkÂ â€¦\n",
      "\n",
      "Help make it happen for C3 Test for Coronavirus & COVID-19 on usermention urllinkÂ \n",
      "\n",
      "usermention I sent you a LinkedIn request. Thanks for your timely information on Covid 19. I admire your work. I admire Mayo Clinic. My wife Mary Ann Edwards is a pharmacist\n",
      "\n",
      "So, not only was the order screwed up, and billing screwed up, The usermention store at the Valley Mall robbed us. And acccording to 611 there is shit we can do about it because they canâ€™t contact the store until the #COVID19 pandemic is over.\n",
      "\n",
      "So you got your #slushfund and now you want people to go to work? You do not understand the risk of COVID19! usermention urllinkÂ â€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(usermention_replace(test5[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0a4fb3cdd046fe81de90745753451b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(usermention_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll turn our attention to **replacing the hashtags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_replace(text):\n",
    "    regex = re.compile(r\"#([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"hashtag\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123 = \"Hey @earny_joe, how are you doing during the #coronavirus #COVID_19 pandemic? #StayHome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey @earny_joe, how are you doing during the hashtag hashtag pandemic? hashtag'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_replace(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# grab 5 observations to showcase hashtag_replace\n",
    "test5 = df[df[\"processed_tweet\"].str.contains(r\"#([^\\s:]+)+\", re.I)][\"processed_tweet\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before `hashtag_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got fired yesterday due to the damage of coronavirus. Not only me, 70 colleagues got fired from my company. People are dying. Companies are closing. CHINA MUST TAKE RESPONSIBILITY FOR THIS!!!!!!! #COVID19 #ChinaVirus\n",
      "\n",
      "NEW #CORONAVIRUS #COVID19 #VETgirl webinar - FREE - starting in 30 minutes, 8:30pm Eastern.   Sign up now!  Get the info YOU need to keep yourself, your practice, your clients, and your patients safe!  #Veterinary #VetMed #VetTech #Pandemic urllinkÂ â€¦\n",
      "\n",
      "You may be hearing a lot of concerning information in the news, and even in these #coronavirus prevention tip videos. But the best way to proceed is to stay calm, stay informed, and know the facts. Learn more here. #COVID19  urllinkÂ \n",
      "\n",
      "honestly? This shit is dark. What the Texas lieutenant governor proposed IN REAL LIFE is what I proposed as an absurd, way over-the-top (because it's so overtly inhumane!) remedy meant to call attention to a real problem  urllinkÂ â€¦ h/t Swift #CoronavirusUSA #COVID19\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! #jasonkenney #COVID19 #N95 #abhealth pictwitter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(test5[n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After `hashtag_replace`...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got fired yesterday due to the damage of coronavirus. Not only me, 70 colleagues got fired from my company. People are dying. Companies are closing. CHINA MUST TAKE RESPONSIBILITY FOR THIS!!!!!!! hashtag hashtag\n",
      "\n",
      "NEW hashtag hashtag hashtag webinar - FREE - starting in 30 minutes, 8:30pm Eastern.   Sign up now!  Get the info YOU need to keep yourself, your practice, your clients, and your patients safe!  hashtag hashtag hashtag hashtag urllinkÂ â€¦\n",
      "\n",
      "You may be hearing a lot of concerning information in the news, and even in these hashtag prevention tip videos. But the best way to proceed is to stay calm, stay informed, and know the facts. Learn more here. hashtag  urllinkÂ \n",
      "\n",
      "honestly? This shit is dark. What the Texas lieutenant governor proposed IN REAL LIFE is what I proposed as an absurd, way over-the-top (because it's so overtly inhumane!) remedy meant to call attention to a real problem  urllinkÂ â€¦ h/t Swift hashtag hashtag\n",
      "\n",
      "Was this photo even taken during the COVID crisis?? ðŸ˜‰ JK is probably more scared of touching poverty and being infected with social services. Drastic non-privatized times call for drastic measures. No N95 for you! hashtag hashtag hashtag hashtag pictwitter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(test5)):\n",
    "    print(hashtag_replace(test5[n]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd19dea3e3e4ce984fcbe9f63e225f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"processed_tweet\"] = df[\"processed_tweet\"].progress_apply(hashtag_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      " 19  processed_tweet     object\n",
      "dtypes: bool(1), int64(7), object(12)\n",
      "memory usage: 516.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to stop here for today (**reference: April 7, 2020 @ 4:19pm PDT**). In the following cell, I'm going to save the dataframe that I have so far into a pickle file - `covid19_0320_0324_updated.pkl` and store in the `playground_data` folder, which will allow me to store data that I've edited but that isn't quite in its final form yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"playground_data/covid19_0320_0324_updated.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
