{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Text Normalization and Tokenization_\n",
    "\n",
    "Machine learning algorithms technically don't work with text data, however, there is a workaround for this. By pre-processing the text, and then converting it into a numerical format (i.e. vectors of numbers), it is then in an appropriate format that can be fed into ML algorithms. But what does text pre-processing entail exactly?  \n",
    "\n",
    "This is where things get interesting. From a high-level, preprocessing removes as much noise as possible from the text data, that way an algorithm can more readily find any potential patterns. But determining what is noise and what is not is significantly impacted by the type of text. For example, you do not want to use the same text pre-processing techniques when you are analyzing Tweets versus when you are analyzing novels. Sure, there may be some overlap, but these two examples of text are significantly different not only in their function but in the text patterns that exist within them (after all, you won't see any emojis in Dostoyevsky's __The Brothers Karamazov__...)\n",
    "\n",
    "That being said there are some central core of text processing strategies that will help you get started:\n",
    "- __Lower casing__: by lowercasing all of the text data, it allows us to capture a word that may have multiple spellings due to miscellaneous uppercasing. For example, a text may include: `America`, `aMerica`, and `AMERICA`. Now we know these are all the same word, however, machines don't, they think these are three different words. To help our computer come to its senses, we lowercase all the text so it can then recognize three cases of `america`, instead of one case of three words. \n",
    "- __Stemming__: This means looking for the \"root\" of a word. There are plenty of words that have multiple inflections. Take the word `connect`; some of its inflections include: `connected`, `connection`, and `connects`. With stemming, we can crudely change the inflection words to the root word by chopping off their endings.\n",
    "- __Lemmatization__: Similar to stemming, in that its goal is to remove inflections, but it does it in a less crude way. It attempts to transform words to the actual root. Take the word `geese`, which is an inflection of `goose`. By using lemmatization, we can change it back into its original root (versus simply chopping off the letters at the end). \n",
    "- __Removing Stopwords__: When you are dealing with text, a lot of the words used actually provide no significant value. Examples include `a`, `this`, `and`, etc. What is the benefit of this? In theory, it allows us to keep only the important words. Lets take a look at the following sentence: `John is going to the store.` Now, let's remove `is`, `to`, and `the`: `John going store`. While it isn't grammatically correct, you still get the primary concept, that John is going to the store. While humans may think it's weird to read the above, this strategy has the potential to help a machine. \n",
    "- __Text Normalization__: Due to the character limits for Tweets, people will often use non-standard forms of words. One such example would be the use of `omg`, which stands for `oh my god`; another example would be the use of `2mrw` as a stand in for the word `tomorrow`. As I mentioned, this is pretty common pattern in social media text, so is a technique to seriously consider for this project. \n",
    "- __Text Enrichment / Augmentation__: Believe it or not, this strategy augments (i.e. adds) information that wasn't previously there before in hopes that can improve its predictive power. Sub-strategies could include things like part-of-speech tagging, or dependency parsing. \n",
    "\n",
    "We'll start by loading in our libraries and the data, after which we'll create some function that help us pre-process the text, including those from the 8.0 notebook (to address text components unique to Tweets):\n",
    "- Links to Twitter pictures, YouTube videos, and other assorted URLs\n",
    "- Mentioning other Twitter users and hashtags\n",
    "- Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import emoji\n",
    "import spacy\n",
    "import string\n",
    "pd.options.display.max_columns = None\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      " 19  processed_tweet     object\n",
      "dtypes: bool(1), int64(7), object(12)\n",
      "memory usage: 516.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"playground_data/covid19_0320_0324_updated_v2.pkl\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Define Functions_\n",
    "\n",
    "The first thing we're going to do is transfer over the functions that we defined in the 8.0 notebook. Below are their names and a short description of what they do:\n",
    "\n",
    "- `newline_remove` --> Given a text as input, it replaces all instances of newline characters `\\n` with a blank space\n",
    "- `twitterpic_replace` --> If there is a link to a picutre in a format similar to `pic.twitter.com/`, we replace that substring with a special token `xxpictwit`\n",
    "- `youtube_replace` --> similar to the above, but geared specifically to any YouTube links, replacing them with `xxyoutubeurl`\n",
    "- `url_replace` --> similar to the above, but geared specifically for any other miscellaneous URLs, replacing them with `xxurl`\n",
    "- `usermention_replace` --> Replaces any user mentions (e.g. `@earny_joe`) in a given text with a special token `xxuser`\n",
    "- `hashtag_replace` --> Replaces any hashtags (e.g. `#DataScience`) in a given text with a special token `xxhashtag`\n",
    "- `emoji_replace` --> Replaces any emojis (e.g. ðŸ˜‰) in a given text with a special token `xxemoji`\n",
    "\n",
    "All the above functions will then be wrapped with the `unique_twitter_tokens` function, meaning they all be performed when its called (versus calling each one of them individually). \n",
    "\n",
    "The next step is to normalize `COVID-19` and, to a lesser extent, `coronavirus`, since there are various forms that people may use in the text. The function, `replace_corona`, searches for any instances of `coronavirus`, including if there is a space between corona and virus, and if only corona is utilized. While there are surely instances where the word `corona` may be used in another context, say Tweets talking about Corona beer, since this dataset was generated using a filter for `coronavirus`, I don't think there'll be too many instances like this. Next the function `replace_covid`, normalizes instances `COVID-19` to `covid19`. Both of these functions attempt to make identifying these central terms easier for later analysis.  \n",
    "\n",
    "After that, we'll define a function called `spacy_tokenizer`. This function will use the [`spaCy`](https://spacy.io/) NLP Python library to tokenize the text. In a nutshell, a tokenizer splits a sentence up unto parts, often with each word representing a part, otherwise known as a \"token.\" This tokenizer will lemmatize all words except pronouns, in additioning to lowercasing and stripping any extra whitespace. This function will be used inside the wrapper function `normalize_text`. In addition to the `replace_covid`, `replace_corona`, and `spacy_tokenizer`, this wrapper function also removes punctuation, stop words, and any lingering extra whitespace. \n",
    "\n",
    "All of the functions above will then be wrapped in a master function of sorts, called `clean_tweet_text`. It'll condense all the above into two lines, by first calling `unique_twitter_tokens` and then `normalize_text`, after which it returns a cleaned up Tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile clean_tweets.py\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "parser = English()\n",
    "\n",
    "# all the functions below (except the wrappers) were taken from the 8.0 Text Preprocessing Notebook\n",
    "def newline_remove(text):\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return regex.sub(\" \", text)\n",
    "\n",
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"xxpictwit\", text)\n",
    "\n",
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"xxyoutubeurl\", text)\n",
    "\n",
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"xxurl\", text)\n",
    "\n",
    "def usermention_replace(text):\n",
    "    regex = re.compile(r\"@([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxuser\", text)\n",
    "\n",
    "def hashtag_replace(text):\n",
    "    regex = re.compile(r\"#([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxhashtag\", text)\n",
    "\n",
    "def emoji_replace(text):\n",
    "    # first demojize text\n",
    "    new_text = emoji.demojize(text, use_aliases=True)\n",
    "    regex = re.compile(r\"(:\\S+:)\", re.I)\n",
    "    return regex.sub(\" xxemoji \", new_text)\n",
    "\n",
    "def unique_twitter_tokens(text):\n",
    "    text = newline_remove(text)\n",
    "    text = twitterpic_replace(text)\n",
    "    text = youtube_replace(text)\n",
    "    text = url_replace(text)\n",
    "    text = usermention_replace(text)\n",
    "    text = hashtag_replace(text)\n",
    "    text = emoji_replace(text)\n",
    "    return text\n",
    "\n",
    "def replace_corona(text):\n",
    "    regex = re.compile(r\"(corona[\\s]?virus)|(corona)\", re.I)\n",
    "    return regex.sub(\"coronavirus\", text)\n",
    "\n",
    "def replace_covid(text):\n",
    "    regex = re.compile(r\"(covid[-\\s]?19)+\", re.I)\n",
    "    return regex.sub(\"covid19\", text)\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    mytokens = parser(text)\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    return \" \".join(mytokens)\n",
    "\n",
    "def normalize_text(text):\n",
    "    # some strategies were inspired by Ryan Kingery's ML with classification tutorial notebook\n",
    "    PUNC = string.punctuation + \"â€¦\" + \"â€“\"\n",
    "    text = replace_covid(text)\n",
    "    text = replace_corona(text)\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # put spaces between punctuation\n",
    "    punct = r\"[\" + re.escape(PUNC) + r\"]\"\n",
    "    text = re.sub(\"(?<! )(?=\" + punct + \")|(?<=\" + punct + \")(?! )\", r\" \", text)\n",
    "    # tokenize\n",
    "    text = spacy_tokenizer(text)\n",
    "    # create string without punctuation & stop words\n",
    "    text = \" \".join([word for word in text.split() if word not in PUNC and word not in STOP_WORDS])\n",
    "    # remove any extra whitespace\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    text = unique_twitter_tokens(text)\n",
    "    text = normalize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Testing Our Text Cleaner_\n",
    "\n",
    "So we've defined our prerequisite functions, now what do we do? We test to see how fast it works. Because the ultimate aim of this project is to be able to detect potential misinformation in near real-time (i.e. as people scroll through their feeds), we need everything to be as quick as possible. This gives us the opportunity to break down everything with `line_profiler`; more specifically, we'll be using the `%lprun` IPython magic. This will give us a line-by-line report of the `clean_tweet_text` function, that'll show us how long each step is taking. If there seem to be any bottlenecks, we can dive deeper from there. \n",
    "\n",
    "Additionally, I made a Python script of the cell above, called `clean_tweets.py` (using another form of IPython magic, `%%makefile`) because `%lprun` tends to [work better when a function is defined in a file](https://ipython-books.github.io/43-profiling-your-code-line-by-line-with-line_profiler/)\n",
    "\n",
    "After that we'll import the script and run `%lprun`, outputting the report in a file named `initial_test`. \n",
    "\n",
    "**PS** - The Tweet text below is from our dataset, with some slight modifications. First, I added an emoji, and second I added some miscellanous newline characters (e.g. `\\n`), to test its ability to replace newline characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_tweets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxuser dear pm happening countrymen pay xxhashtag tests xxemoji unbelievable xxpictwit\n"
     ]
    }
   ],
   "source": [
    "testtweet = \"\"\"@narendramodi Dear PM, Is this really happening?\\n\\nThe countrymen have to pay for #Covid19 Tests?  \n",
    "ðŸ§ Unbelievable! \\n pic.twitter.com/31nvInjcBZ\n",
    "\"\"\"\n",
    "print(clean_tweet_text(testtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 Âµs Â± 12.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# how long, on average does the function take to run?\n",
    "%timeit clean_tweet_text(testtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'initial_test'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000841 s\n",
      "File: /notebooks/covid_disinfo_detect/experiments/clean_tweets.py\n",
      "Function: clean_tweet_text at line 86\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    86                                           def clean_tweet_text(text):\n",
      "    87         1        459.0    459.0     54.6      text = unique_twitter_tokens(text)\n",
      "    88         1        381.0    381.0     45.3      text = normalize_text(text)\n",
      "    89         1          1.0      1.0      0.1      return text\n"
     ]
    }
   ],
   "source": [
    "%lprun -T initial_test -f clean_tweet_text clean_tweet_text(testtweet)\n",
    "print(open(\"initial_test\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `unique_twitter_tokens` function is taking up the majority of the time. We can focus in on it specifically with `%lprun` to see what the holdup might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'unique_twitter_tokens_test'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000406 s\n",
      "File: /notebooks/covid_disinfo_detect/experiments/clean_tweets.py\n",
      "Function: unique_twitter_tokens at line 47\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    47                                           def unique_twitter_tokens(text):\n",
      "    48         1         24.0     24.0      5.9      text = newline_remove(text)\n",
      "    49         1         14.0     14.0      3.4      text = twitterpic_replace(text)\n",
      "    50         1         13.0     13.0      3.2      text = youtube_replace(text)\n",
      "    51         1         12.0     12.0      3.0      text = url_replace(text)\n",
      "    52         1          9.0      9.0      2.2      text = usermention_replace(text)\n",
      "    53         1          8.0      8.0      2.0      text = hashtag_replace(text)\n",
      "    54         1        325.0    325.0     80.0      text = emoji_replace(text)\n",
      "    55         1          1.0      1.0      0.2      return text\n"
     ]
    }
   ],
   "source": [
    "%lprun -T unique_twitter_tokens_test -f unique_twitter_tokens unique_twitter_tokens(testtweet)\n",
    "print(open(\"unique_twitter_tokens_test\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like one component - `emoji_replace` - is the culprit. I'm going to see if we can dive further into this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'emoji_replace'. \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.00064 s\n",
      "File: /notebooks/covid_disinfo_detect/experiments/clean_tweets.py\n",
      "Function: emoji_replace at line 41\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    41                                           def emoji_replace(text):\n",
      "    42                                               # first demojize text\n",
      "    43         1        618.0    618.0     96.6      new_text = emoji.demojize(text, use_aliases=True)\n",
      "    44         1         10.0     10.0      1.6      regex = re.compile(r\"(:\\S+:)+\", re.I)\n",
      "    45         1         12.0     12.0      1.9      return regex.sub(\" xxemoji \", new_text)\n"
     ]
    }
   ],
   "source": [
    "%lprun -T emoji_replace -f emoji_replace emoji_replace(testtweet)\n",
    "print(open(\"emoji_replace\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Text preprocessing v2_\n",
    "\n",
    "This section is still experimental; it is attempting to improve upon the above text pre-processing done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtweet = \"\"\"Sars-Cov-2 @narendramodi Dear PM, Is this really happening?\\n\\nThe countrymen have to pay for COVID-19 \n",
    "corona virus #Covid19 Tests?  ðŸ§ Unbelievable! \\n #coronavirus pic.twitter.com/31nvInjcBZ \n",
    "https://www.youtube.com/watch?v=ig9yh8iVZWI\n",
    "https://github.com/rkingery/ml_tutorials/blob/master/notebooks/ml_with_text.ipynb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# 1\n",
    "def newline_remove(text):\n",
    "    regex = re.compile(r'\\n+', re.I)\n",
    "    text = regex.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "# 2\n",
    "def replace_coronavirus(text):\n",
    "    regex = re.compile(r'(corona[\\s]?virus)', re.I)\n",
    "    return regex.sub('coronavirus', text)\n",
    "\n",
    "# 3\n",
    "def coronavirus_hashtags(text):\n",
    "    regex = re.compile(r'#(coronavirus)\\b', re.I)\n",
    "    return regex.sub('xxhashcoronavirus', text)\n",
    "\n",
    "# 4\n",
    "def replace_covid(text):\n",
    "    regex = re.compile(r'(covid[-\\s_]?19)', re.I)\n",
    "    return regex.sub('covid19', text)\n",
    "\n",
    "# 5\n",
    "def covid_hashtags(text):\n",
    "    regex = re.compile(r'#(covid[_-]?(19))', re.I)\n",
    "    return regex.sub('xxhashcovid19', text)\n",
    "\n",
    "# 6\n",
    "def sarscov2_replace(text):\n",
    "    regex = re.compile(r'(sars[-]?cov[-]?2)', re.I)\n",
    "    return regex.sub(r'sarscov2', text)\n",
    "\n",
    "# \n",
    "def emoji_replace(text):\n",
    "    # first demojize text\n",
    "    new_text = emoji.demojize(text, use_aliases=True)\n",
    "    regex = re.compile(r\"(:\\S+:)\", re.I)\n",
    "    return regex.sub(\" xxemoji \", new_text)\n",
    "\n",
    "# 7\n",
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"xxpictwit\", text)\n",
    "\n",
    "# \n",
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"xxyoutubeurl\", text)\n",
    "\n",
    "# \n",
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"xxurl\", text)\n",
    "\n",
    "#re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')\n",
    "\n",
    "\n",
    "# \n",
    "def punctuation_space(text):\n",
    "    # put spaces between punctuation\n",
    "    PUNC = string.punctuation + \"â€¦\" + \"â€“\"\n",
    "    punct = r\"[\" + re.escape(PUNC) + r\"]\"\n",
    "    text = re.sub(\"(?<! )(?=\" + punct + \")|(?<=\" + punct + \")(?! )\", r\" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\",' xxpunct ',text)\n",
    "    # remove any extra whitespace\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text\n",
    "\n",
    "# wrapper\n",
    "def wrapper(text):\n",
    "    text = newline_remove(text)\n",
    "    text = replace_coronavirus(text)\n",
    "    text = coronavirus_hashtags(text)\n",
    "    text = replace_covid(text)\n",
    "    text = covid_hashtags(text)\n",
    "    text = sarscov2_replace(text)\n",
    "    text = text.replace(r'#', '')\n",
    "    text = text.replace(r'@', '')\n",
    "    text = emoji_replace(text)\n",
    "    #text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    text = twitterpic_replace(text)\n",
    "    text = youtube_replace(text)\n",
    "    text = url_replace(text)\n",
    "    #text = punctuation_space(text)\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    # text = p.tokenize(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123 = \"corona virus, \\n\\ncorona, \\n coronavirus, #coronavirus, #corona, #coronavirus5g\"\n",
    "test345 = \"covid19,!k \\ncovid-19,  \\n\\n COVID_19, #covid19, #covid-19 #COVID_19, #covid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus,  corona,   coronavirus, xxhashcoronavirus, #corona, #coronavirus5g'"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coronavirus_hashtags(replace_coronavirus(newline_remove(test123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19,!k  covid19,    covid19, xxhashcovid19, xxhashcovid19 xxhashcovid19, #covid'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_hashtags(replace_covid(newline_remove(test345)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus, corona, coronavirus, xxhashcoronavirus, corona, coronavirus5g'"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19,!k covid19, covid19, xxhashcovid19, xxhashcovid19 xxhashcovid19, covid'"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(test345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sarscov2 narendramodi Dear PM, Is this really happening? The countrymen have to pay for covid19 coronavirus xxhashcovid19 Tests? xxemoji Unbelievable! xxhashcoronavirus xxpictwit xxyoutubeurl xxurl'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(testtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@narendramodi Dear PM, Is this really happening?\\n\\nThe countrymen have to pay for COVID-19 \\ncorona virus #Covid19 Tests?  ðŸ§ Unbelievable! \\n #coronavirus pic.twitter.com/31nvInjcBZ \\nhttps://www.youtube.com/watch?v=ig9yh8iVZWI\\nhttps://github.com/rkingery/ml_tutorials/blob/master/notebooks/ml_with_text.ipynb\\n'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2 = '''\n",
    "Iâ€™ll â€œstay away from anyone you love or respectâ€. \n",
    "\n",
    "Will you stay away from the 5G towers that emulate Covid19 symptoms? \n",
    "\n",
    "Will you research the 5G rollout in Wuhan late last year? \n",
    "\n",
    "Will you learn that 5G causes respiratory failure by inhibiting hemoglobin-oxygen transfer?\n",
    "\n",
    "#5G\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iâ€™ll â€œstay away from anyone you love or respectâ€. Will you stay away from the 5G towers that emulate covid19 symptoms? Will you research the 5G rollout in Wuhan late last year? Will you learn that 5G causes respiratory failure by inhibiting hemoglobin-oxygen transfer? 5G'"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sarscov2'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sarscov2_replace(text):\n",
    "    regex = re.compile(r'(sars[-]?cov[-]?2)', re.I)\n",
    "    return regex.sub(r'sarscov2', text)\n",
    "\n",
    "sarscov2_replace('SARS-CoV-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123 = '#corona, #Coronavirus, #CoronavirusTrump, #CoronaTrump, corona virus, coronavirus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # corona , # Coronavirus , # CoronavirusTrump , # CoronaTrump , corona virus , coronavirus'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_space(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus coronavirus coronavirus coronavirus corona virus, coronavirus'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coronavirus_hashtags(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test123 = '#COVID_19, #covid-19, #covid19, #covid_19, COVID19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19, covid19, covid19, covid19, COVID19'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_hashtags(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus coronavirus coronavirus coronavirus coronavirus , coronavirus'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(test123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$MENTION$ Dear PM, Is this really happening? The countrymen have to pay for covid19 Tests? ðŸ§ Unbelievable! $URL$'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper(testtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$MENTION$ Dear PM, Is this really happening? The countrymen have to pay for covid19 Tests? ðŸ§ Unbelievable! $URL$'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.tokenize(wrapper(testtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$RESERVED$ Preprocessor is $HASHTAG$ $EMOJI$ $URL$'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.tokenize('RT Preprocessor is #awesome ðŸ‘ https://github.com/s/preprocessor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New update to make sure git is set up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be continued...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Experimenting w/ [`tweet-preprocessor 0.5.0`](https://pypi.org/project/tweet-preprocessor/)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
